{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae02499",
   "metadata": {},
   "source": [
    "# ROBERTA sequential classifier as simple QA system\n",
    "\n",
    "This notebook is the code necessary to finetune a ROBERTA sequence classifier as a QA system on our provided training and testing data.\n",
    "\n",
    "The data is comprised of zoning ordinance questions and their respective answers.\n",
    "\n",
    "This system was not designed to be a full fledged QA system but is created as a contrast to the more fully featured systems tested in other notebooks and implementations. Specifically a ROBERTA model was required by the finetuning of the SQuAD based model in the next notebooks and therefore we selected ROBERTA here to have a direct comparison. \n",
    "\n",
    "To run this notebook simply run each cell in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fef62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "file_path = f'{os.getcwd()}/data'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77693e",
   "metadata": {},
   "source": [
    "### Tokenization and creation of Hugging Face Dataset class object\n",
    "\n",
    "The ROBERTA model and tokenizer from Hugging Face require that the data be converted to a dataset object, hence the need for the train/test split to exist as json\n",
    "\n",
    "Since we have already created the json train/test split previously in the BERT notebook we can skip data examination and additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3696039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9c1c5b960549139bf782830ec5962f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8c8c66ad1942ca99ff187796d622da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c83111821a4dd2bcd6a17a244dbfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ffb13f45da879356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/data/user/home/jesusaur/cs662-qa-land-dev-law-sys/programs/data/json/QAZoningTrain.json', 'test': '/data/user/home/jesusaur/cs662-qa-land-dev-law-sys/programs/data/json/QAZoningTest.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/jesusaur/.cache/huggingface/datasets/json/default-ffb13f45da879356/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e438a66c1134c54a627eee8815be5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 955\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('lexlms/roberta-base-uncased', lower=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "data_files = {\"train\": f'{file_path}/json/QAZoningTrain.json', \"test\": f'{file_path}/json/QAZoningTest.json'} # * this is how to load multiple files, need to sklearn train_test_split into two sets first\n",
    "print(data_files)\n",
    "QA_dataset = load_dataset('json', data_files=data_files)\n",
    "print(QA_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80dadbb",
   "metadata": {},
   "source": [
    "### Training and Evaluation Parameters\n",
    "\n",
    "Initialization of the pretrained model and tokenization of dataset.\n",
    "\n",
    "These are the parameters used for training and evaluation in the process of finetuning the model.\n",
    "\n",
    "Metrics selected were Accuracy and F1 from the Hugging Face Evaluate library.\n",
    "\n",
    "The model is evaluated and saved at each epoch.\n",
    "\n",
    "There is opportunity for additional hyperparameter tuning at this stage but results were adequate using these initial parameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4de73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc023886d89241f790c3233ef05e8601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec4d3edfea046448f77ddf501442042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e346b14c9aba4c519b968a2d8ad7ca0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/692 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339c48689d934f7db55984fb6e34e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at lexlms/roberta-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at lexlms/roberta-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = QA_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained('lexlms/roberta-base-uncased', num_labels=48)\n",
    "\n",
    "metric1 = evaluate.load('f1')\n",
    "metric2 = evaluate.load('accuracy')\n",
    "\n",
    "training_args = TrainingArguments(output_dir = \"test_trainer\",\n",
    "                                  evaluation_strategy = \"epoch\",\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                  do_train=True,\n",
    "                                  do_eval=True,\n",
    "                                  learning_rate=1e-5,\n",
    "                                  logging_steps=50,\n",
    "                                  eval_steps=50,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  per_device_eval_batch_size=8,\n",
    "                                  num_train_epochs=25,\n",
    "                                  weight_decay=0.001,)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metric1.compute(predictions=predictions, references=labels, average='macro')\n",
    "    accuracy = metric2.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy['accuracy'], \"f1\": f1['f1']}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c398c",
   "metadata": {},
   "source": [
    "Fine tunining the pretrained model begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc77c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jesusaur/.conda/envs/NLP-SPARQL/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 955\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "Trainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22, 'LABEL_23': 23, 'LABEL_24': 24, 'LABEL_25': 25, 'LABEL_26': 26, 'LABEL_27': 27, 'LABEL_28': 28, 'LABEL_29': 29, 'LABEL_30': 30, 'LABEL_31': 31, 'LABEL_32': 32, 'LABEL_33': 33, 'LABEL_34': 34, 'LABEL_35': 35, 'LABEL_36': 36, 'LABEL_37': 37, 'LABEL_38': 38, 'LABEL_39': 39, 'LABEL_40': 40, 'LABEL_41': 41, 'LABEL_42': 42, 'LABEL_43': 43, 'LABEL_44': 44, 'LABEL_45': 45, 'LABEL_46': 46, 'LABEL_47': 47}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 24:00, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.957200</td>\n",
       "      <td>1.550662</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.052844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.258800</td>\n",
       "      <td>1.228285</td>\n",
       "      <td>0.682243</td>\n",
       "      <td>0.130226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.095100</td>\n",
       "      <td>1.149370</td>\n",
       "      <td>0.682243</td>\n",
       "      <td>0.132505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.976600</td>\n",
       "      <td>1.067050</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.210750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.860700</td>\n",
       "      <td>0.936458</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.310231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>0.783563</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.325286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.719934</td>\n",
       "      <td>0.850467</td>\n",
       "      <td>0.412667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.633422</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.459615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.564662</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.484984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>0.513802</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.440947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.468340</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.468162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.423614</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.475514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.396639</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.519632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.375206</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.349035</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.341922</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.338351</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.205300</td>\n",
       "      <td>0.322816</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.313372</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.311426</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.199500</td>\n",
       "      <td>0.304792</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>0.301951</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>0.300526</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.299480</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.647619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.298939</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.647619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-120\n",
      "Configuration saved in test_trainer/checkpoint-120/config.json\n",
      "Model weights saved in test_trainer/checkpoint-120/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-240\n",
      "Configuration saved in test_trainer/checkpoint-240/config.json\n",
      "Model weights saved in test_trainer/checkpoint-240/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-360\n",
      "Configuration saved in test_trainer/checkpoint-360/config.json\n",
      "Model weights saved in test_trainer/checkpoint-360/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-480\n",
      "Configuration saved in test_trainer/checkpoint-480/config.json\n",
      "Model weights saved in test_trainer/checkpoint-480/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-600\n",
      "Configuration saved in test_trainer/checkpoint-600/config.json\n",
      "Model weights saved in test_trainer/checkpoint-600/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-720\n",
      "Configuration saved in test_trainer/checkpoint-720/config.json\n",
      "Model weights saved in test_trainer/checkpoint-720/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-840\n",
      "Configuration saved in test_trainer/checkpoint-840/config.json\n",
      "Model weights saved in test_trainer/checkpoint-840/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-960\n",
      "Configuration saved in test_trainer/checkpoint-960/config.json\n",
      "Model weights saved in test_trainer/checkpoint-960/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1080\n",
      "Configuration saved in test_trainer/checkpoint-1080/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1080/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1200\n",
      "Configuration saved in test_trainer/checkpoint-1200/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1200/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1320\n",
      "Configuration saved in test_trainer/checkpoint-1320/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1320/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1440\n",
      "Configuration saved in test_trainer/checkpoint-1440/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1440/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1560\n",
      "Configuration saved in test_trainer/checkpoint-1560/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1560/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1680\n",
      "Configuration saved in test_trainer/checkpoint-1680/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1680/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1800\n",
      "Configuration saved in test_trainer/checkpoint-1800/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1800/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in test_trainer/checkpoint-1920/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1920/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2040\n",
      "Configuration saved in test_trainer/checkpoint-2040/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2040/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2160\n",
      "Configuration saved in test_trainer/checkpoint-2160/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2160/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2280\n",
      "Configuration saved in test_trainer/checkpoint-2280/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2280/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2400\n",
      "Configuration saved in test_trainer/checkpoint-2400/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2400/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2520\n",
      "Configuration saved in test_trainer/checkpoint-2520/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2520/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2640\n",
      "Configuration saved in test_trainer/checkpoint-2640/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2640/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2760\n",
      "Configuration saved in test_trainer/checkpoint-2760/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2760/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2880\n",
      "Configuration saved in test_trainer/checkpoint-2880/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2880/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-3000\n",
      "Configuration saved in test_trainer/checkpoint-3000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.5237573135693868, metrics={'train_runtime': 1443.5754, 'train_samples_per_second': 16.539, 'train_steps_per_second': 2.078, 'total_flos': 6284370917376000.0, 'train_loss': 0.5237573135693868, 'epoch': 25.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214cf41",
   "metadata": {},
   "source": [
    "Evaluation occurs during training but addition of this call to evaluate() allows us to print the best model's final metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189521d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29893869161605835,\n",
       " 'eval_accuracy': 0.9345794392523364,\n",
       " 'eval_f1': 0.6476190476190476,\n",
       " 'eval_runtime': 1.9873,\n",
       " 'eval_samples_per_second': 53.841,\n",
       " 'eval_steps_per_second': 7.045,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71419fd7",
   "metadata": {},
   "source": [
    "Recreating the dictionary from the BERT notebook for use with the mapping function to convert class labels back into their natural language counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03d8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = {'No': 1, 'Yes': 2, \"['0 [ft_i]']\": 3, \"['1 [du/acr_u]']\": 4, \"['10 [ft_i]']\": 5, \"['100 [ft_i]']\": 6,\n",
    " \"['10000 [sft_i]']\": 7, \"['12 [du/acr_u]']\": 8, \"['12 [u/acr_u]']\": 9, \"['125 [ft_i]']\": 10, \"['15 [ft_i]']\": 11,\n",
    " \"['150 [ft_i]']\": 12, \"['2 [du/acr_u]']\": 13, \"['20 [ft_i]']\": 14, \"['20000 [sft_i]']\": 15, \"['25 [ft_i]']\": 16,\n",
    " \"['30 [ft_i]']\": 17, \"['35 [ft_i]']\": 18, \"['35000 [sft_i]']\": 19, \"['4 [du/acr_u]']\": 20, \"['40 [ft_i]']\": 21,\n",
    " \"['5 [ft_i]']\": 22, \"['50 [ft_i]']\": 23, \"['6 [du/acr_u]']\": 24, \"['60 [ft_i]']\": 25, \"['6000 [sft_i]']\": 26,\n",
    " \"['70 [ft_i]']\": 27, \"['75 [ft_i]']\": 28, \"['8 [du/acr_u]']\": 29, \"['80 [ft_i]']\": 30, \"['90 [ft_i]']\": 31,\n",
    " \"['A1']\": 32, \"['A2']\": 33, \"['C1', 'C2', 'C3', 'C4', 'FI1', 'FI2', 'FI3']\": 34, \"['C1', 'C2', 'C3', 'C4']\": 35,\n",
    " \"['C2', 'C3', 'C4']\": 36, \"['C3', 'C4']\": 37, \"['C4']\": 38, \"['FI1', 'FI2', 'FI3']\": 39, \"['FI2', 'FI3']\": 40,\n",
    " \"['FI3']\": 41, \"['R1', 'R2', 'R3', 'C1', 'C2', 'C3', 'C4', 'FI1', 'FI2', 'FI3']\": 42,\n",
    " \"['R1', 'R2', 'R3', 'C1', 'C2', 'C3', 'C4']\": 43, \"['R1', 'R2', 'R3']\": 44, \"['R2', 'R3']\": 45, \"['R3']\": 46,\n",
    " '[]': 47}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ccc9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(d, value):\n",
    "   return [k for k, v in d.items() if v == value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b6ad6",
   "metadata": {},
   "source": [
    "### Checking some results:\n",
    "\n",
    "Below is a sanity check to see what kind of results are returned after the model is trained.\n",
    "\n",
    "We have provided one incorrect and one correct result but feel free to explore using the same format to find additional examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750f5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('What is the minimum lot size in the R2a zoning district?',\n",
       " [\"['6000 [sft_i]']\"],\n",
       " [\"['10000 [sft_i]']\"])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = trainer.predict(tokenized_data[\"test\"])\n",
    "results = (tokenized_data[\"test\"][4]['text'], get_key(conversion, np.argmax(prediction[0][4], axis=-1)), \n",
    "          get_key(conversion, prediction[1][4]))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3351559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Are research or testing laboratories allowed in a FI2 zoning district?',\n",
       " ['Yes'],\n",
       " ['Yes'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = trainer.predict(tokenized_data[\"test\"])\n",
    "results = (tokenized_data[\"test\"][2]['text'], get_key(conversion, np.argmax(prediction[0][2], axis=-1)), \n",
    "          get_key(conversion, prediction[1][2]))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658e56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:NLP-SPARQL]",
   "language": "python",
   "name": "conda-env-NLP-SPARQL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
