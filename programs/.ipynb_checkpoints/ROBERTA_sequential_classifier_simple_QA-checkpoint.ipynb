{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35fef62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "file_path = f'{os.getcwd()}/data'\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3696039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 385/385 [00:00<00:00, 516kB/s]\n",
      "Downloading: 100%|██████████| 2.18M/2.18M [00:00<00:00, 18.3MB/s]\n",
      "Downloading: 100%|██████████| 167/167 [00:00<00:00, 249kB/s]\n",
      "loading file tokenizer.json from cache at /home/jesusaur/.cache/huggingface/hub/models--lexlms--roberta-base-uncased/snapshots/098511e0b42988cf6b882d3828feab9f58f0e0b7/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jesusaur/.cache/huggingface/hub/models--lexlms--roberta-base-uncased/snapshots/098511e0b42988cf6b882d3828feab9f58f0e0b7/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jesusaur/.cache/huggingface/hub/models--lexlms--roberta-base-uncased/snapshots/098511e0b42988cf6b882d3828feab9f58f0e0b7/tokenizer_config.json\n",
      "Using custom data configuration default-f147e92c9891d68a\n",
      "Found cached dataset json (/home/jesusaur/.cache/huggingface/datasets/json/default-f147e92c9891d68a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '/data/user/home/jesusaur/cs662-qa-land-dev-law-sys/programs/data/json/QAZoningTrain.json', 'test': '/data/user/home/jesusaur/cs662-qa-land-dev-law-sys/programs/data/json/QAZoningTest.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 903.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 955\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 107\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('lexlms/roberta-base-uncased', lower=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "data_files = {\"train\": f'{file_path}/json/QAZoningTrain.json', \"test\": f'{file_path}/json/QAZoningTest.json'} # * this is how to load multiple files, need to sklearn train_test_split into two sets first\n",
    "print(data_files)\n",
    "QA_dataset = load_dataset('json', data_files=data_files)\n",
    "print(QA_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d4de73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jesusaur/.cache/huggingface/datasets/json/default-f147e92c9891d68a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7145b3aacefc0b95.arrow\n",
      "Loading cached processed dataset at /home/jesusaur/.cache/huggingface/datasets/json/default-f147e92c9891d68a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-55903fc0f41c026c.arrow\n",
      "loading configuration file config.json from cache at /home/jesusaur/.cache/huggingface/hub/models--lexlms--roberta-base-uncased/snapshots/098511e0b42988cf6b882d3828feab9f58f0e0b7/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"lexlms/roberta-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jesusaur/.cache/huggingface/hub/models--lexlms--roberta-base-uncased/snapshots/098511e0b42988cf6b882d3828feab9f58f0e0b7/pytorch_model.bin\n",
      "Some weights of the model checkpoint at lexlms/roberta-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at lexlms/roberta-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = QA_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained('lexlms/roberta-base-uncased', num_labels=48)\n",
    "\n",
    "metric1 = evaluate.load('f1')\n",
    "metric2 = evaluate.load('accuracy')\n",
    "\n",
    "training_args = TrainingArguments(output_dir = \"test_trainer\",\n",
    "                                  evaluation_strategy = \"epoch\",\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                  do_train=True,\n",
    "                                  do_eval=True,\n",
    "                                  learning_rate=1e-5,\n",
    "                                  logging_steps=50,\n",
    "                                  eval_steps=50,\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  per_device_eval_batch_size=8,\n",
    "                                  num_train_epochs=25,\n",
    "                                  weight_decay=0.001,)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metric1.compute(predictions=predictions, references=labels, average='macro')\n",
    "    accuracy = metric2.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy['accuracy'], \"f1\": f1['f1']}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bc77c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jesusaur/.conda/envs/NLP-SPARQL/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 955\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 124479792\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 23:29, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.913500</td>\n",
       "      <td>1.480998</td>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.066866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.256200</td>\n",
       "      <td>1.216446</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.107790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.099900</td>\n",
       "      <td>1.140747</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.153503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>1.078076</td>\n",
       "      <td>0.719626</td>\n",
       "      <td>0.173363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.917300</td>\n",
       "      <td>0.973338</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.269557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.840290</td>\n",
       "      <td>0.831776</td>\n",
       "      <td>0.400444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.675400</td>\n",
       "      <td>0.735515</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.398667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.638531</td>\n",
       "      <td>0.850467</td>\n",
       "      <td>0.427619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.536800</td>\n",
       "      <td>0.563233</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.507692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.511818</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.529012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.456817</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.563580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.414250</td>\n",
       "      <td>0.906542</td>\n",
       "      <td>0.547279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.388710</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.536310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.365155</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.582993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.350162</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.582993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.327897</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.582993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.582993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.312090</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.582993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.205600</td>\n",
       "      <td>0.304619</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.301110</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.296325</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.683951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.290724</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.683951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.288304</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.683951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.287487</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.683951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.225900</td>\n",
       "      <td>0.287187</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.683951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-120\n",
      "Configuration saved in test_trainer/checkpoint-120/config.json\n",
      "Model weights saved in test_trainer/checkpoint-120/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-240\n",
      "Configuration saved in test_trainer/checkpoint-240/config.json\n",
      "Model weights saved in test_trainer/checkpoint-240/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-360\n",
      "Configuration saved in test_trainer/checkpoint-360/config.json\n",
      "Model weights saved in test_trainer/checkpoint-360/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-480\n",
      "Configuration saved in test_trainer/checkpoint-480/config.json\n",
      "Model weights saved in test_trainer/checkpoint-480/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-600\n",
      "Configuration saved in test_trainer/checkpoint-600/config.json\n",
      "Model weights saved in test_trainer/checkpoint-600/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-720\n",
      "Configuration saved in test_trainer/checkpoint-720/config.json\n",
      "Model weights saved in test_trainer/checkpoint-720/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-840\n",
      "Configuration saved in test_trainer/checkpoint-840/config.json\n",
      "Model weights saved in test_trainer/checkpoint-840/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-960\n",
      "Configuration saved in test_trainer/checkpoint-960/config.json\n",
      "Model weights saved in test_trainer/checkpoint-960/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1080\n",
      "Configuration saved in test_trainer/checkpoint-1080/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1080/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1200\n",
      "Configuration saved in test_trainer/checkpoint-1200/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1200/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1320\n",
      "Configuration saved in test_trainer/checkpoint-1320/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1320/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1440\n",
      "Configuration saved in test_trainer/checkpoint-1440/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1440/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1560\n",
      "Configuration saved in test_trainer/checkpoint-1560/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1560/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1680\n",
      "Configuration saved in test_trainer/checkpoint-1680/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1680/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1800\n",
      "Configuration saved in test_trainer/checkpoint-1800/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1800/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in test_trainer/checkpoint-1920/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1920/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2040\n",
      "Configuration saved in test_trainer/checkpoint-2040/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2040/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2160\n",
      "Configuration saved in test_trainer/checkpoint-2160/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2160/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2280\n",
      "Configuration saved in test_trainer/checkpoint-2280/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2280/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2400\n",
      "Configuration saved in test_trainer/checkpoint-2400/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2400/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2520\n",
      "Configuration saved in test_trainer/checkpoint-2520/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2520/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2640\n",
      "Configuration saved in test_trainer/checkpoint-2640/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2640/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2760\n",
      "Configuration saved in test_trainer/checkpoint-2760/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2760/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2880\n",
      "Configuration saved in test_trainer/checkpoint-2880/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2880/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-3000\n",
      "Configuration saved in test_trainer/checkpoint-3000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.5392564558982849, metrics={'train_runtime': 1410.4466, 'train_samples_per_second': 16.927, 'train_steps_per_second': 2.127, 'total_flos': 6284370917376000.0, 'train_loss': 0.5392564558982849, 'epoch': 25.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "189521d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2871874272823334,\n",
       " 'eval_accuracy': 0.9439252336448598,\n",
       " 'eval_f1': 0.6839506172839507,\n",
       " 'eval_runtime': 1.9336,\n",
       " 'eval_samples_per_second': 55.337,\n",
       " 'eval_steps_per_second': 7.24,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a03d8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion = {'No': 1, 'Yes': 2, \"['0 [ft_i]']\": 3, \"['1 [du/acr_u]']\": 4, \"['10 [ft_i]']\": 5, \"['100 [ft_i]']\": 6,\n",
    " \"['10000 [sft_i]']\": 7, \"['12 [du/acr_u]']\": 8, \"['12 [u/acr_u]']\": 9, \"['125 [ft_i]']\": 10, \"['15 [ft_i]']\": 11,\n",
    " \"['150 [ft_i]']\": 12, \"['2 [du/acr_u]']\": 13, \"['20 [ft_i]']\": 14, \"['20000 [sft_i]']\": 15, \"['25 [ft_i]']\": 16,\n",
    " \"['30 [ft_i]']\": 17, \"['35 [ft_i]']\": 18, \"['35000 [sft_i]']\": 19, \"['4 [du/acr_u]']\": 20, \"['40 [ft_i]']\": 21,\n",
    " \"['5 [ft_i]']\": 22, \"['50 [ft_i]']\": 23, \"['6 [du/acr_u]']\": 24, \"['60 [ft_i]']\": 25, \"['6000 [sft_i]']\": 26,\n",
    " \"['70 [ft_i]']\": 27, \"['75 [ft_i]']\": 28, \"['8 [du/acr_u]']\": 29, \"['80 [ft_i]']\": 30, \"['90 [ft_i]']\": 31,\n",
    " \"['A1']\": 32, \"['A2']\": 33, \"['C1', 'C2', 'C3', 'C4', 'FI1', 'FI2', 'FI3']\": 34, \"['C1', 'C2', 'C3', 'C4']\": 35,\n",
    " \"['C2', 'C3', 'C4']\": 36, \"['C3', 'C4']\": 37, \"['C4']\": 38, \"['FI1', 'FI2', 'FI3']\": 39, \"['FI2', 'FI3']\": 40,\n",
    " \"['FI3']\": 41, \"['R1', 'R2', 'R3', 'C1', 'C2', 'C3', 'C4', 'FI1', 'FI2', 'FI3']\": 42,\n",
    " \"['R1', 'R2', 'R3', 'C1', 'C2', 'C3', 'C4']\": 43, \"['R1', 'R2', 'R3']\": 44, \"['R2', 'R3']\": 45, \"['R3']\": 46,\n",
    " '[]': 47}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "46ccc9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(d, value):\n",
    "   return [k for k, v in d.items() if v == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "750f5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('What is the minimum lot size in the R2a zoning district?',\n",
       " [\"['6000 [sft_i]']\"],\n",
       " [\"['10000 [sft_i]']\"])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = trainer.predict(tokenized_data[\"test\"])\n",
    "result = (tokenized_data[\"test\"][4]['text'], get_key(conversion, np.argmax(prediction[0][4], axis=-1)), \n",
    "          get_key(conversion, prediction[1][4]))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3351559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 107\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Are research or testing laboratories allowed in a FI2 zoning district?',\n",
       " ['Yes'],\n",
       " ['Yes'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = trainer.predict(tokenized_data[\"test\"])\n",
    "results = (tokenized_data[\"test\"][2]['text'], get_key(conversion, np.argmax(prediction[0][2], axis=-1)), \n",
    "          get_key(conversion, prediction[1][2]))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658e56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-NLP-SPARQL]",
   "language": "python",
   "name": "conda-env-.conda-NLP-SPARQL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
